---
title: "ANLY512 Assignment 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####**5.4.5**
<br>

```{r Q5}

# library(ISLR)
# library(dplyr)
# 
# data("Default")
# head(Default)
# 
# table(Default$default)
# 
# Default_balanced <- tail(Default[order(Default$default),], -(9667-333))
# table(Default_balanced$default)
# 
# set.seed(1)
# 
# default <- glm(default ~ income + balance, data = Default, family = "binomial")
# summary(default)
# 
# default_pred <- predict(default, data = Default_balanced, type ="response")
# 
# train <- sample(nrow(Default_balanced),(nrow(Default_balanced) * .75))
# 
# default1 <- glm(default ~ income + balance, data = Default_balanced, family = "binomial", subset = train)
# 
# Default_Validate <- Default_balanced[-train]
# default1_pred <- predict(default1, data = Default_Validate, type ="response")
# 
# Default_Validate$class_pred_1 <- predict(default1_pred, data = Default_Validate, type = "response") > .5
# 
# Default_Validate$misclassified_1 <- ifelse(Default_balanced$default==Default$class_pred_1,1,0)
# 
# default2 <- glm(default ~ income + balance, data = Default_balanced, family = "binomial", subset = train)
# default2_pred <- predict(default2, data = Default_balanced, type ="response")
# Default$class_pred_2 <- predict(default2, data = Default_balanced, type ="response") > 0.5
# Default$misclassified_2 <- ifelse(Default$default==Default$class_pred_2,1,0)
# 
# default3 <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
# default3_pred <- predict(default3, data = Default, type ="response")
# Default$class_pred_3 <- predict(default3, data = Default, type ="response") > 0.5
# Default$misclassified_3 <- ifelse(Default$default==Default$class_pred_3,1,0) 
# 
# 


```

####**5.4.5**
<br>


```{r Q9}

# (a)

library(MASS)
library(boot)
data("Boston")
head(Boston)

mu_hat <- mean(Boston$medv)
mu_hat

se_mu <- (sd(Boston$medv)/sqrt(dim(Boston)))
se_mu

set.seed(2)

boot.fn <- function(data, index) {
mu_hat <- mean(data[index])
return (mu_hat)
}

boot(Boston$medv, boot.fn, 1000)

```

The population mean estimate is 22.53 and the SD is 0.41.

Using bootstrap we get an estimated mean of 22.53 with a Standard Error of 0.41. Thse values are very close to our estimates for the population mean and SD.

####**Extra Problem 38**

Suppose we are given a training set with n observations and want to conduct k-fold cross-validation. Assume always that n=km where m is an integer.

(a). In k-fold cross validation we divide our data into k pieces and then use one of these as a validation set, with the remaining sets representing training data. If k = 2, then there will be 1 training set and 1 validation set, with the observations being split into half for each. Thus the only way to partition the data into 2 folds is all the ways we can split the data into half, which is represented by the given equation.

(b). For k = 3, we will have 1 validation set and 2 training sets for the data. The n!/m!m!m! portion of our equation is a representation of all the possible combinations for dividing our observations into 3 different sets. We also divide by three to signify the number of folds.

(c). 1/k * (n!/k(m!^k))

####**Extra Problem 39**
<br>


```{r Extra39, echo=FALSE}

library(readr)
library(boot)
library(leaps)

Advertising <- read_csv("Advertising.csv", 
    col_types = cols(X1 = col_skip()))

sales1 <- glm(Sales ~ ., data = Advertising)
cv.err=cv.glm(Advertising, sales1)
cv.err$delta

# Create models

model1 <- glm(Sales ~ TV, data= Advertising)
model2 <- glm(Sales ~ Radio, data= Advertising)
model3 <- glm(Sales ~ Newspaper, data= Advertising)
model4 <- glm(Sales ~ TV + Newspaper, data= Advertising)
model5 <- glm(Sales ~ TV + Radio, data= Advertising)
model6 <- glm(Sales ~ Radio + Newspaper, data= Advertising)
model7 <- glm(Sales ~ TV + Radio + Newspaper, data= Advertising)

anova(model1, model2, model3, model4, model5, model6, model7)

```

From the analysis we can see that TV and Radio help explain a greater share of the variation in sales, whereas newspapers do not seem to be effective advertisement mechanisms. There is not a big difference in the validation error for Model5 and Model7. TV adverisement seems to be an effective mechanism as the validation error amount akways falls when we add it to the model.

####**Extra Problem 40**
<br>


```{r Extra40}

library(readr)
library(boot)
library(leaps)

Advertising <- read_csv("Advertising.csv", 
    col_types = cols(X1 = col_skip()))

# Radio

cv.error=rep(0,2)
for (i in 1:2){
  glm.salesRadio <- glm(Sales ~ poly(Radio ,i) + Newspaper + TV, data=Advertising)
  cv.error[i]=cv.glm(Advertising, glm.salesRadio)$delta[1]}

cv.error

# TV

cv.error=rep(0,2)
for (i in 1:2){
  glm.salesTV <- glm(Sales ~ poly(TV ,i) + Newspaper + Radio, data=Advertising)
  cv.error[i]=cv.glm(Advertising, glm.salesTV)$delta[1]}

cv.error

# Newspaper

cv.error=rep(0,2)
for (i in 1:2){
  glm.salesNews <- glm(Sales ~ poly(Newspaper ,i) + Radio + TV, data=Advertising)
  cv.error[i]=cv.glm(Advertising, glm.salesNews)$delta[1]}

cv.error


```

We should include an interaction term for TV as the validation error goes down if we include it. However for the other two interaction terms we see that the MSE stays the same and hence there is no benefit in including them in the model.


####**Extra Problem 41**
<br>


```{r Extra41}

data(cars)
head(cars)

library(boot)

# LOOCV

cv.error=rep(0,5)
for (i in 1:5){
  glm.dist <- glm(dist ~ poly(speed ,i), data=cars)
  cv.error[i]=cv.glm(cars, glm.dist)$delta[1]}

cv.error

# 10 Fold CV

set.seed(2)

cv.error.10=rep(0,10)

for (i in 1:10){
  glm.dist2 <- glm(dist ~ poly(speed ,i), data=cars)
  cv.error.10[i]=cv.glm(cars,glm.dist2,K=10)$delta[1]
  }
cv.error.10

```
With LOOCV we see a drop in MSE when we use a quadratic term, but adding a cubic term or higher to the model actually causes the test MSE to rise. This indicates that the best model for predicted stopping distance from speed only uses a polynomial term to the order of 2.

On the other the 10 fold cross validation shows a decrease with the use of a quadratic term as well but not higher. In this case we also conclude that we should include a quadratic term in the model only.
