---
title: "ANLY 512 Assignment 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
####**3.7.3**
<br>
We can rewrite this regression model as follows:

\[ Y = 50 + 20(GPA) + 0.07(IQ) + 35(Gender) + 0.01 (GPA*IQ) - 10(Gender*GPA) \]

(a). For this question, option (ii) is the correct one. 
The coding for gender is equal to 1 if the individual is female. As Beta 3 is 35 and positive, it implies that holding everything else constant women earn $35,000 more on average than men (y-intercept is higher by this amount).

(b). \[ Salary = 50 + 20*4 + 0.07*110 + 35*1 + 0.01*4*110 - 10*1*4 \]

\[ Salary = $ 137,100\]

(c). False. The Beta(4) term is small (0.01) but the units on the salary are in 1000s of a dollar, so this coefficient actually implies an increase of $10. Furthermore this term signifies the different in slope between IQ and GPA and even a small value can make a difference. In order to decide whether or not an interaction effect exists we should check the p-value of Beta(4) to see if it is significant or not.

####**3.7.8**
<br>
```{r}
library(ISLR)
data(Auto)
head(Auto)
summary(Auto)

model <- lm(mpg ~ horsepower, data = Auto)

summary(model)

predict(model, data.frame(horsepower = 98), interval = "confidence")

```

(a). There is a relationship between the predictor (horsepower) and our response variable (mpg). For the cars in our dataset for a 1 unit increase in horsepower there is a decrease in fuel efficiency of 0.158 mpg on average. The relationship is very significant implying that this is a strong relationship.

For bhp = 98, the expected fuel efficiency is 39.935861 - 0.157845*(98) = 24.47 mpg.

(b). & (c).

```{r plotone, echo=FALSE}

plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower (bhp)", ylab = "Fuel Efficiency (mpg)")
abline(39.935861, -0.157845)

par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model)

```


####**3.7.10**

<br>

```{r}

data(Carseats)
head(Carseats)

model2 <- lm(Sales ~ Price + Urban + US , data = Carseats)
summary(model2)

```

(b) The Intercept coefficient has a value of 13.043469 implying that 13,043 carseats are sold in a rural store outside of the US which a price of zero.
The Price coefficient has a value of -0.054459 which implies that holding everything else constant, an increase of $1 in the price of carseats results in a decrease of 54.5 in seats sold.
The Urban coefficient has a value of -0.021916 and this implies that holding everything else constant, Urban stores sell 22 car seats fewer on average.
The US coefficient has a value of 1.200573 and this implies that holding everything else constant, Urban stores sell 1200 car seats more on average.

(c) \[ Y = 13.04 - 0.05X1 - 0.02X2 + 1.20X3 \]

\[ where X1 = Price, X2 = Dummy for Urban & X3 = Dummy for US \]


####**3.7.15**
<br>

```{r}

library(MASS)
data(Boston)
head(Boston)

storage <- list()

for(i in names(Boston)[-1]){
  storage[[i]] <- lm(crim ~ get(i), Boston)
  print(storage[[i]])
}


# (b)

summary(lm(crim ~ ., Boston))

```

We can reject the null hypothesis for the coefficients on dis(weighted mean of distances to five Boston employment centres) and rad(index of accessibility to radial highways) at the 0.1% level. We can reject the null hypthosis for medv(median value of owner-occupied homes in $1000s) at the 1% confidence level.
We can reject the null hypothesis for black(000(Bk - 0.63)^2 where Bk is the proportion of blacks by town) and zn(proportion of residential land zoned for lots over 25,000 sq.ft.) at the 5% confidence level.


####**Extra Problem 10**
<br>

```{r}

data(cars)
head(cars)

plot(cars$speed, cars$dist)

cars_model<- lm(dist ~ speed, cars) 

```
<br>

####**3.7.9**
(a).
```{r plottwo, echo=FALSE}

pairs(Auto)

```

(b).

```{r}

Auto_values <- Auto[,-9]

cov(Auto_values, use="complete.obs")

summary(Auto_model <- lm(mpg ~ ., Auto_values))

```

(c)(i). Yes there is a relationship between some of the predictors and the response value. From our correlation matrix we could already notice some patterns that emerge when we compare mpg to the predictor values.

(ii). After running a regression we see that weight, year and origin are significant at the 0.1% level and displacement is significant at the 1% level. The relationship between mpg and these predictors is positive except for weight.

(iii). The coefficient for year implies that for an increase in 1 unit for year (i.e. as each year passes) the average mpg that a car does goes up by 0.75 units.

```{r}

par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(Auto_model)

```

(d).

There are a few points with high residuals and the residual plot is slightly curved. Also point 14 has exceptionally high leverage. However on the whole there are not a lot of extreme values apart from the few mentioned.

(e). & (f).

```{r}

summary(lm(mpg ~ displacement + year + weight + origin + weight*year + displacement*year, Auto))

summary(lm(mpg ~ displacement + year + weight + origin + I(displacement^2) + I(horsepower^2) + I(weight^2), Auto))

summary(lm(mpg ~ displacement + year + weight + origin + I(sqrt(displacement)) + I(sqrt(horsepower)) + I(sqrt(weight)), Auto))

summary(lm(mpg ~ displacement + year + weight + origin + I(log(displacement)) + I(log(horsepower)) + I(log(weight)), Auto))

```

We observe that certain variables, for example horsepower, which are not significant in a linear form are significant in the model if included in a log or sqrt(x) form.

####**Extra Problems 14**
<br>

```{r}

beta_0 <-1
beta_1 <- -0.5
x_t <- 2
X <- matrix(nrow = 100, ncol = 2)


for(i in seq(0,100,1)){
  
  x_t <- beta_0 + beta_1*x_t + rnorm(1, mean = 0, sd = 0.2)
  X[i,1] <- i
  X[i,2] <- x_t
  
}

plot(X[,1],X[,2], xlab = "t", ylab = "x")
lines(X[,1],X[,2])

ts_1 <- as.ts(X[,2])
plot(ts_1)

# (b)

beta_0 <-1
beta_1 <- 0.5
x_t <- 2
X2 <- matrix(nrow = 100, ncol = 2)


for(i in seq(0,100,1)){
  
  x_t <- beta_0 + beta_1*x_t + rnorm(1, mean = 0, sd = 0.2)
  X2[i,1] <- i
  X2[i,2] <- x_t
  
}

plot(X2[,1],X2[,2], xlab = "t", ylab = "x")
lines(X2[,1],X2[,2])

ts_2 <- as.ts(X[,2])
plot(ts_2)

# (c)

beta_0 <-1
beta_1 <- -0.9
x_t <- 2
X3 <- matrix(nrow = 100, ncol = 2)


for(i in seq(0,100,1)){
  
  x_t <- beta_0 + beta_1*x_t + rnorm(1, mean = 0, sd = 0.2)
  X3[i,1] <- i
  X3[i,2] <- x_t
  
}

plot(X3[,1],X3[,2], xlab = "t", ylab = "x")
lines(X3[,1],X3[,2])

ts_2 <- as.ts(X3[,2])
plot(ts_2)

```


####**Extra Problems 15**
<br>

```{r}

library(dplyr)

beta_0 <-1
beta_1 <- -0.5
x_t <- 2
X4 <- matrix(nrow = 100, ncol = 3)


for(i in seq(0,100,1)){
  
  x_t <- beta_0 + beta_1*x_t + rnorm(1, mean = 0, sd = 0.2)
  X4[i,1] <- i
  X4[i,2] <- x_t
  
}

X4[,3] <- lag(X4[,2],1)

plot(X4[,2],X4[,3], xlab = "X(t-1)", ylab = "X(t)")

df <- as.data.frame(X4)
df <- subset(df, select = -c(V1) )

colnames(df)[colnames(df)=="V2"] <- "xt"
colnames(df)[colnames(df)=="V3"] <- "xt_1"

summary(lm(xt ~ .,df))


```


